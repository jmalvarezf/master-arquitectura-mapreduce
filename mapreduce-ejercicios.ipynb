{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Ejercicios Map-Reduce Master Bigdata </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/mrjob/ejercicio1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/mrjob/ejercicio1\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanuatu,VU\r",
      "\r\n",
      "\"Venezuela, Bolivarian Republic of\",VE\r",
      "\r\n",
      "Viet Nam,VN\r",
      "\r\n",
      "\"Virgin Islands, British\",VG\r",
      "\r\n",
      "\"Virgin Islands, U.S.\",VI\r",
      "\r\n",
      "Wallis and Futuna,WF\r",
      "\r\n",
      "Western Sahara,EH\r",
      "\r\n",
      "Yemen,YE\r",
      "\r\n",
      "Zambia,ZM\r",
      "\r\n",
      "Zimbabwe,ZW\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! tail -10 countries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isis Sorrells  ,regular,US\r\n",
      "Chester Abdul  ,regular,ES\r\n",
      "Manda Wingate  ,regular,SI\r\n",
      "Anna Rappold  ,regular,SB\r\n",
      "Albina Lamore  ,malo,SO\r\n",
      "Carolyn Machado  ,bueno,ZA\r\n",
      "Jeni Espinoza  ,bueno,GS\r\n",
      "Charisse Salzman  ,bueno,SS\r\n",
      "Dorla Silber  ,bueno,ES\r\n",
      "Lilli Bryson  ,malo,LK\r\n"
     ]
    }
   ],
   "source": [
    "! tail -10 clients.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob-ejercicio1_alt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio1_alt.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    resultMap = {}\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "            if country[1] in resultMap:\n",
    "                resultMap[country[1]] += value[1]\n",
    "            else:\n",
    "                resultMap[country[1]] = value[1]\n",
    "    for key in resultMap:\n",
    "        yield [key], resultMap[key]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/mrjob-ejercicio1_alt.root.20180115.202804.359487\n",
      "Streaming final output from /tmp/mrjob-ejercicio1_alt.root.20180115.202804.359487/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1_alt.root.20180115.202804.359487...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1_alt.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Canada\"]\t1\r\n",
      "[\"Somalia\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n",
      "[\"South Africa\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio1.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, [value]])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield ([country[1]], sum(value[1]))\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield (key, sum(values))\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 2...\n",
      "Creating temp directory /tmp/mrjob-ejercicio1.root.20180115.200410.058182\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/mrjob-ejercicio1.root.20180115.200410.058182/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1.root.20180115.200410.058182...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Spain\"]\t3\r\n",
      "[\"Guinea\"]\t1\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Canada\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 18:56 /tmp/master-map-reduce-ejercicio1/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 18:56 /tmp/master-map-reduce-ejercicio1/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20180115.185719.976023\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20180115.185719.976023/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob7106172340203674728.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516037010436_0003\n",
      "  Submitted application application_1516037010436_0003\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0003/\n",
      "  Running job: job_1516037010436_0003\n",
      "  Job job_1516037010436_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20180115.185719.976023/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=503361\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19667968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2738176\n",
      "\t\tTotal time spent by all map tasks (ms)=19207\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19207\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2674\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2674\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19207\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2674\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3740\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1094\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1506050048\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=1783103488\n",
      "\t\tVirtual memory (bytes) snapshot=10613653504\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob8433989118519982262.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516037010436_0004\n",
      "  Submitted application application_1516037010436_0004\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0004/\n",
      "  Running job: job_1516037010436_0004\n",
      "  Job job_1516037010436_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0004 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio1-output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=211\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367566\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=714\n",
      "\t\tHDFS: Number of bytes written=211\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8667136\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2688000\n",
      "\t\tTotal time spent by all map tasks (ms)=8464\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8464\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2625\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2625\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8464\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2625\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1750\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=219\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=810868736\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=752353280\n",
      "\t\tVirtual memory (bytes) snapshot=7959044096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio1-output...\n",
      "[\"Canada\"]\t1\n",
      "[\"Guam\"]\t3\n",
      "[\"Guinea\"]\t1\n",
      "[\"Portugal\"]\t1\n",
      "[\"Qatar\"]\t1\n",
      "[\"Somalia\"]\t1\n",
      "[\"South Africa\"]\t1\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\n",
      "[\"South Sudan\"]\t1\n",
      "[\"Spain\"]\t3\n",
      "[\"Turkey\"]\t1\n",
      "[\"United States\"]\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20180115.185719.976023...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20180115.185719.976023...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1.py hdfs:///tmp/master-map-reduce-ejercicio1/* --output-dir hdfs:///tmp/carpeta/ejercicio1-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Canada\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n",
      "[\"Guinea\"]\t1\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Somalia\"]\t1\r\n",
      "[\"South Africa\"]\t1\r\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Spain\"]\t3\r\n",
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio1-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio2.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield ([country[1]], sum([value[1]]))\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield None, [sum(values), key]\n",
    " \n",
    "  def reducer_third(self, _, values):\n",
    "    yield max(values)\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second),\n",
    "        MRStep(reducer=self.reducer_third)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 3...\n",
      "Creating temp directory /tmp/mrjob-ejercicio2.root.20180115.190554.166971\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/mrjob-ejercicio2.root.20180115.190554.166971/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio2.root.20180115.190554.166971...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio2.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal_ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal_ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 19:06 /tmp/master-map-reduce-ejercicio2/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 19:06 /tmp/master-map-reduce-ejercicio2/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio2.root.20180115.190707.295761\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180115.190707.295761/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob6772644701095248390.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516037010436_0005\n",
      "  Submitted application application_1516037010436_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0005/\n",
      "  Running job: job_1516037010436_0005\n",
      "  Job job_1516037010436_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180115.190707.295761/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=503417\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19181568\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2760704\n",
      "\t\tTotal time spent by all map tasks (ms)=18732\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18732\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2696\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2696\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18732\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2696\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1215\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1495756800\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=1839202304\n",
      "\t\tVirtual memory (bytes) snapshot=10609926144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob3326740913576798192.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516037010436_0006\n",
      "  Submitted application application_1516037010436_0006\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0006/\n",
      "  Running job: job_1516037010436_0006\n",
      "  Job job_1516037010436_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0006 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180115.190707.295761/step-output/0001\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367773\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8089600\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2828288\n",
      "\t\tTotal time spent by all map tasks (ms)=7900\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7900\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2762\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2762\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7900\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2762\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2390\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=338\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1001459712\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=1180172288\n",
      "\t\tVirtual memory (bytes) snapshot=7961591808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob169947065701456242.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516037010436_0007\n",
      "  Submitted application application_1516037010436_0007\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0007/\n",
      "  Running job: job_1516037010436_0007\n",
      "  Job job_1516037010436_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0007 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio2-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=367686\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=791\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8406016\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2683904\n",
      "\t\tTotal time spent by all map tasks (ms)=8209\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8209\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2621\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2621\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8209\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2621\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1920\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=212\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=811003904\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=816316416\n",
      "\t\tVirtual memory (bytes) snapshot=7960207360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio2-output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180115.190707.295761...\n",
      "Removing temp directory /tmp/mrjob-ejercicio2.root.20180115.190707.295761...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio2.py hdfs:///tmp/master-map-reduce-ejercicio2/* --output-dir hdfs:///tmp/carpeta/ejercicio2-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio2-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio3.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield ([country[1]], sum([value[1]]))\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield None, [sum(values), key]\n",
    " \n",
    "  def reducer_third(self, key, values):\n",
    "        valuesAgain = []\n",
    "        maxq = 1\n",
    "        for value in values:\n",
    "            if value[0] >= maxq:\n",
    "                maxq = value[0]\n",
    "            valuesAgain.append(value)\n",
    "        for value in valuesAgain:\n",
    "            if value[0] >= maxq:\n",
    "                yield maxq, value[1]\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second),\n",
    "        MRStep(reducer=self.reducer_third)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 3...\n",
      "Creating temp directory /tmp/mrjob-ejercicio3.root.20180115.201739.204068\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/mrjob-ejercicio3.root.20180115.201739.204068/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio3.root.20180115.201739.204068...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio3.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal_ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\r\n",
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal_ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 20:18 /tmp/master-map-reduce-ejercicio3/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 20:18 /tmp/master-map-reduce-ejercicio3/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio3.root.20180115.201908.155987\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180115.201908.155987/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob2156341250762303722.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516037010436_0008\n",
      "  Submitted application application_1516037010436_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0008/\n",
      "  Running job: job_1516037010436_0008\n",
      "  Job job_1516037010436_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180115.201908.155987/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=503417\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18671616\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2747392\n",
      "\t\tTotal time spent by all map tasks (ms)=18234\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18234\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2683\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2683\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18234\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2683\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3160\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=662\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1310367744\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=1468530688\n",
      "\t\tVirtual memory (bytes) snapshot=10614439936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob1744705838740869610.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516037010436_0009\n",
      "  Submitted application application_1516037010436_0009\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0009/\n",
      "  Running job: job_1516037010436_0009\n",
      "  Job job_1516037010436_0009 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0009 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180115.201908.155987/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367773\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8651776\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2639872\n",
      "\t\tTotal time spent by all map tasks (ms)=8449\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8449\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2578\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2578\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8449\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2578\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2620\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=414\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1012232192\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=1182793728\n",
      "\t\tVirtual memory (bytes) snapshot=7961083904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob2421033984309649257.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516037010436_0010\n",
      "  Submitted application application_1516037010436_0010\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516037010436_0010/\n",
      "  Running job: job_1516037010436_0010\n",
      "  Job job_1516037010436_0010 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516037010436_0010 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio3-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=367689\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=791\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8080384\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2703360\n",
      "\t\tTotal time spent by all map tasks (ms)=7891\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7891\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2640\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2640\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7891\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2640\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1910\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=243\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=810201088\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=815267840\n",
      "\t\tVirtual memory (bytes) snapshot=7961894912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio3-output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\n",
      "3\t[\"Spain\"]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180115.201908.155987...\n",
      "Removing temp directory /tmp/mrjob-ejercicio3.root.20180115.201908.155987...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio3.py hdfs:///tmp/master-map-reduce-ejercicio3/* --output-dir hdfs:///tmp/carpeta/ejercicio3-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\r\n",
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio3-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
