{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Ejercicios Map-Reduce Master Bigdata </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/mrjob/ejercicio1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/mrjob/ejercicio1\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanuatu,VU\r",
      "\r\n",
      "\"Venezuela, Bolivarian Republic of\",VE\r",
      "\r\n",
      "Viet Nam,VN\r",
      "\r\n",
      "\"Virgin Islands, British\",VG\r",
      "\r\n",
      "\"Virgin Islands, U.S.\",VI\r",
      "\r\n",
      "Wallis and Futuna,WF\r",
      "\r\n",
      "Western Sahara,EH\r",
      "\r\n",
      "Yemen,YE\r",
      "\r\n",
      "Zambia,ZM\r",
      "\r\n",
      "Zimbabwe,ZW\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! tail -10 countries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isis Sorrells  ,regular,US\r\n",
      "Chester Abdul  ,regular,ES\r\n",
      "Manda Wingate  ,regular,SI\r\n",
      "Anna Rappold  ,regular,SB\r\n",
      "Albina Lamore  ,malo,SO\r\n",
      "Carolyn Machado  ,bueno,ZA\r\n",
      "Jeni Espinoza  ,bueno,GS\r\n",
      "Charisse Salzman  ,bueno,SS\r\n",
      "Dorla Silber  ,bueno,ES\r\n",
      "Lilli Bryson  ,malo,LK\r\n"
     ]
    }
   ],
   "source": [
    "! tail -10 clients.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio1_alt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio1_alt.py\n",
    "import sys, os, re, csv\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    reader = csv.reader([line], delimiter=',', quotechar='\"')\n",
    "    splits = []\n",
    "    for r in reader:\n",
    "        if len(r) == 2:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "        else:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "            splits.append(r[2])\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    resultMap = {}\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "            if country[1] in resultMap:\n",
    "                resultMap[country[1]] += value[1]\n",
    "            else:\n",
    "                resultMap[country[1]] = value[1]\n",
    "    for key in resultMap:\n",
    "        yield [key], resultMap[key]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/mrjob-ejercicio1_alt.root.20180123.162023.510174\n",
      "Streaming final output from /tmp/mrjob-ejercicio1_alt.root.20180123.162023.510174/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1_alt.root.20180123.162023.510174...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1_alt.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Canada\"]\t1\r\n",
      "[\"Somalia\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n",
      "[\"South Africa\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio1.py\n",
    "import sys, os, re, csv\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    reader = csv.reader([line], delimiter=',', quotechar='\"')\n",
    "    splits = []\n",
    "    for r in reader:\n",
    "        if len(r) == 2:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "        else:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "            splits.append(r[2])\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield [country[1]], value[1]\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield (key, sum(values))\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 2...\n",
      "Creating temp directory /tmp/mrjob-ejercicio1.root.20180123.162209.278196\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/mrjob-ejercicio1.root.20180123.162209.278196/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1.root.20180123.162209.278196...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Spain\"]\t3\r\n",
      "[\"Guinea\"]\t1\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Canada\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 18:56 /tmp/master-map-reduce-ejercicio1/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 18:56 /tmp/master-map-reduce-ejercicio1/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/ejercicio1-output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /tmp/carpeta/ejercicio1-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio1.root.20180123.162232.137253\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio1.root.20180123.162232.137253/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob8989323183883426722.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516719309696_0001\n",
      "  Submitted application application_1516719309696_0001\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0001/\n",
      "  Running job: job_1516719309696_0001\n",
      "  Job job_1516719309696_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio1.root.20180123.162232.137253/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7636\n",
      "\t\tFILE: Number of bytes written=504925\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25922560\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3383296\n",
      "\t\tTotal time spent by all map tasks (ms)=25315\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25315\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3304\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3304\n",
      "\t\tTotal vcore-seconds taken by all map tasks=25315\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3304\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3400\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=705\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=7098\n",
      "\t\tMap output materialized bytes=7648\n",
      "\t\tMap output records=266\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1299791872\n",
      "\t\tReduce input groups=262\n",
      "\t\tReduce input records=266\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=7648\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=532\n",
      "\t\tTotal committed heap usage (bytes)=1479016448\n",
      "\t\tVirtual memory (bytes) snapshot=10609143808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob2790452933820004108.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516719309696_0002\n",
      "  Submitted application application_1516719309696_0002\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0002/\n",
      "  Running job: job_1516719309696_0002\n",
      "  Job job_1516719309696_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0002 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio1-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=211\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367613\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=211\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11243520\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2944000\n",
      "\t\tTotal time spent by all map tasks (ms)=10980\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10980\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2875\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2875\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10980\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2875\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2310\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=330\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1001332736\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=1115160576\n",
      "\t\tVirtual memory (bytes) snapshot=7961198592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio1-output...\n",
      "[\"Canada\"]\t1\n",
      "[\"Guam\"]\t3\n",
      "[\"Guinea\"]\t1\n",
      "[\"Portugal\"]\t1\n",
      "[\"Qatar\"]\t1\n",
      "[\"Somalia\"]\t1\n",
      "[\"South Africa\"]\t1\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\n",
      "[\"South Sudan\"]\t1\n",
      "[\"Spain\"]\t3\n",
      "[\"Turkey\"]\t1\n",
      "[\"United States\"]\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio1.root.20180123.162232.137253...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1.root.20180123.162232.137253...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1.py hdfs:///tmp/master-map-reduce-ejercicio1/* --output-dir hdfs:///tmp/carpeta/ejercicio1-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Canada\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n",
      "[\"Guinea\"]\t1\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Somalia\"]\t1\r\n",
      "[\"South Africa\"]\t1\r\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Spain\"]\t3\r\n",
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio1-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio2.py\n",
    "import sys, os, re, csv\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    reader = csv.reader([line], delimiter=',', quotechar='\"')\n",
    "    splits = []\n",
    "    for r in reader:\n",
    "        if len(r) == 2:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "        else:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "            splits.append(r[2])\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield [country[1]], value[1]\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield None, [sum(values), key]\n",
    " \n",
    "  def reducer_third(self, _, values):\n",
    "    yield max(values)\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second),\n",
    "        MRStep(reducer=self.reducer_third)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 3...\n",
      "Creating temp directory /tmp/mrjob-ejercicio2.root.20180123.162432.371155\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/mrjob-ejercicio2.root.20180123.162432.371155/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio2.root.20180123.162432.371155...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio2.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal_ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal_ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 19:06 /tmp/master-map-reduce-ejercicio2/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 19:06 /tmp/master-map-reduce-ejercicio2/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/ejercicio2-output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /tmp/carpeta/ejercicio2-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio2.root.20180123.162458.462251\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180123.162458.462251/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob912531982172073045.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516719309696_0003\n",
      "  Submitted application application_1516719309696_0003\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0003/\n",
      "  Running job: job_1516719309696_0003\n",
      "  Job job_1516719309696_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180123.162458.462251/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7636\n",
      "\t\tFILE: Number of bytes written=504921\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19249152\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2665472\n",
      "\t\tTotal time spent by all map tasks (ms)=18798\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18798\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2603\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2603\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18798\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2603\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3710\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1218\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=7098\n",
      "\t\tMap output materialized bytes=7648\n",
      "\t\tMap output records=266\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1687515136\n",
      "\t\tReduce input groups=262\n",
      "\t\tReduce input records=266\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=7648\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=532\n",
      "\t\tTotal committed heap usage (bytes)=2207252480\n",
      "\t\tVirtual memory (bytes) snapshot=10609889280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob966074222951127945.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516719309696_0004\n",
      "  Submitted application application_1516719309696_0004\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0004/\n",
      "  Running job: job_1516719309696_0004\n",
      "  Job job_1516719309696_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0004 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180123.162458.462251/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367770\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8400896\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2824192\n",
      "\t\tTotal time spent by all map tasks (ms)=8204\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8204\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2758\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2758\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8204\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2758\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1980\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=222\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=810090496\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=815267840\n",
      "\t\tVirtual memory (bytes) snapshot=7962636288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob7340542177325861115.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516719309696_0005\n",
      "  Submitted application application_1516719309696_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0005/\n",
      "  Running job: job_1516719309696_0005\n",
      "  Job job_1516719309696_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0005 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio2-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=367689\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=791\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8706048\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2813952\n",
      "\t\tTotal time spent by all map tasks (ms)=8502\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8502\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2748\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2748\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8502\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2748\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=246\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=813682688\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=765460480\n",
      "\t\tVirtual memory (bytes) snapshot=7961919488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio2-output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180123.162458.462251...\n",
      "Removing temp directory /tmp/mrjob-ejercicio2.root.20180123.162458.462251...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio2.py hdfs:///tmp/master-map-reduce-ejercicio2/* --output-dir hdfs:///tmp/carpeta/ejercicio2-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio2-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio3.py\n",
    "import sys, os, re, csv\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    reader = csv.reader([line], delimiter=',', quotechar='\"')\n",
    "    splits = []\n",
    "    for r in reader:\n",
    "        if len(r) == 2:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "        else:\n",
    "            splits.append(r[0])\n",
    "            splits.append(r[1])\n",
    "            splits.append(r[2])\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield [country[1]], value[1]\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield None, [sum(values), key]\n",
    " \n",
    "  def reducer_third(self, key, values):\n",
    "        valuesAgain = []\n",
    "        maxq = 1\n",
    "        for value in values:\n",
    "            if value[0] >= maxq:\n",
    "                maxq = value[0]\n",
    "            valuesAgain.append(value)\n",
    "        for value in valuesAgain:\n",
    "            if value[0] >= maxq:\n",
    "                yield maxq, value[1]\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second),\n",
    "        MRStep(reducer=self.reducer_third)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 3...\n",
      "Creating temp directory /tmp/mrjob-ejercicio3.root.20180123.162650.401325\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/mrjob-ejercicio3.root.20180123.162650.401325/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio3.root.20180123.162650.401325...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio3.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal_ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\r\n",
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal_ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 20:18 /tmp/master-map-reduce-ejercicio3/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 20:18 /tmp/master-map-reduce-ejercicio3/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/ejercicio3-output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /tmp/carpeta/ejercicio3-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio3.root.20180123.162707.769851\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180123.162707.769851/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob2946271969611816244.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516719309696_0006\n",
      "  Submitted application application_1516719309696_0006\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0006/\n",
      "  Running job: job_1516719309696_0006\n",
      "  Job job_1516719309696_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0006 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180123.162707.769851/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7636\n",
      "\t\tFILE: Number of bytes written=504925\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18658304\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2873344\n",
      "\t\tTotal time spent by all map tasks (ms)=18221\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18221\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2806\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2806\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18221\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2806\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3150\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=663\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=7098\n",
      "\t\tMap output materialized bytes=7648\n",
      "\t\tMap output records=266\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1320644608\n",
      "\t\tReduce input groups=262\n",
      "\t\tReduce input records=266\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=7648\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=532\n",
      "\t\tTotal committed heap usage (bytes)=1479016448\n",
      "\t\tVirtual memory (bytes) snapshot=10614706176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob748285224367965913.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516719309696_0007\n",
      "  Submitted application application_1516719309696_0007\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0007/\n",
      "  Running job: job_1516719309696_0007\n",
      "  Job job_1516719309696_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0007 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180123.162707.769851/step-output/0001\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367770\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8056832\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2821120\n",
      "\t\tTotal time spent by all map tasks (ms)=7868\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7868\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2755\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2755\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7868\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2755\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2060\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=229\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=807309312\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=812646400\n",
      "\t\tVirtual memory (bytes) snapshot=7959986176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob5440246122078858369.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516719309696_0008\n",
      "  Submitted application application_1516719309696_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516719309696_0008/\n",
      "  Running job: job_1516719309696_0008\n",
      "  Job job_1516719309696_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516719309696_0008 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio3-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=367689\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=791\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8170496\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2726912\n",
      "\t\tTotal time spent by all map tasks (ms)=7979\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7979\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2663\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2663\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7979\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2663\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2360\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=393\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1001242624\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=1171783680\n",
      "\t\tVirtual memory (bytes) snapshot=7961100288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio3-output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\n",
      "3\t[\"Spain\"]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180123.162707.769851...\n",
      "Removing temp directory /tmp/mrjob-ejercicio3.root.20180123.162707.769851...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio3.py hdfs:///tmp/master-map-reduce-ejercicio3/* --output-dir hdfs:///tmp/carpeta/ejercicio3-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\r\n",
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio3-output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
