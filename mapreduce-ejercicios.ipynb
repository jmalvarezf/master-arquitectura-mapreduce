{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Ejercicios Map-Reduce Master Bigdata </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/mrjob/ejercicio1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/mrjob/ejercicio1\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanuatu,VU\r",
      "\r\n",
      "\"Venezuela, Bolivarian Republic of\",VE\r",
      "\r\n",
      "Viet Nam,VN\r",
      "\r\n",
      "\"Virgin Islands, British\",VG\r",
      "\r\n",
      "\"Virgin Islands, U.S.\",VI\r",
      "\r\n",
      "Wallis and Futuna,WF\r",
      "\r\n",
      "Western Sahara,EH\r",
      "\r\n",
      "Yemen,YE\r",
      "\r\n",
      "Zambia,ZM\r",
      "\r\n",
      "Zimbabwe,ZW\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! tail -10 countries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isis Sorrells  ,regular,US\r\n",
      "Chester Abdul  ,regular,ES\r\n",
      "Manda Wingate  ,regular,SI\r\n",
      "Anna Rappold  ,regular,SB\r\n",
      "Albina Lamore  ,malo,SO\r\n",
      "Carolyn Machado  ,bueno,ZA\r\n",
      "Jeni Espinoza  ,bueno,GS\r\n",
      "Charisse Salzman  ,bueno,SS\r\n",
      "Dorla Silber  ,bueno,ES\r\n",
      "Lilli Bryson  ,malo,LK\r\n"
     ]
    }
   ],
   "source": [
    "! tail -10 clients.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob-ejercicio1_alt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio1_alt.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    resultMap = {}\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "            if country[1] in resultMap:\n",
    "                resultMap[country[1]] += value[1]\n",
    "            else:\n",
    "                resultMap[country[1]] = value[1]\n",
    "    for key in resultMap:\n",
    "        yield [key], resultMap[key]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/mrjob-ejercicio1_alt.root.20180116.192751.489433\n",
      "Streaming final output from /tmp/mrjob-ejercicio1_alt.root.20180116.192751.489433/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1_alt.root.20180116.192751.489433...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1_alt.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Canada\"]\t1\r\n",
      "[\"Somalia\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n",
      "[\"South Africa\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio1.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield [country[1]], value[1]\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield (key, sum(values))\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 2...\n",
      "Creating temp directory /tmp/mrjob-ejercicio1.root.20180116.192903.949863\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/mrjob-ejercicio1.root.20180116.192903.949863/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1.root.20180116.192903.949863...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Spain\"]\t3\r\n",
      "[\"Guinea\"]\t1\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Canada\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 18:56 /tmp/master-map-reduce-ejercicio1/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 18:56 /tmp/master-map-reduce-ejercicio1/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/ejercicio1-output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /tmp/carpeta/ejercicio1-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio1.root.20180116.193036.744895\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio1.root.20180116.193036.744895/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob4110828645117263948.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516130695543_0001\n",
      "  Submitted application application_1516130695543_0001\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0001/\n",
      "  Running job: job_1516130695543_0001\n",
      "  Job job_1516130695543_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio1.root.20180116.193036.744895/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=503417\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=31722496\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3677184\n",
      "\t\tTotal time spent by all map tasks (ms)=30979\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30979\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3591\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3591\n",
      "\t\tTotal vcore-seconds taken by all map tasks=30979\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3591\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=723\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1307832320\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=1291321344\n",
      "\t\tVirtual memory (bytes) snapshot=10615394304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob6406794448925395174.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516130695543_0002\n",
      "  Submitted application application_1516130695543_0002\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0002/\n",
      "  Running job: job_1516130695543_0002\n",
      "  Job job_1516130695543_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0002 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio1-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=211\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367613\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=211\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8542208\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2887680\n",
      "\t\tTotal time spent by all map tasks (ms)=8342\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8342\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2820\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2820\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8342\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2820\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=223\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=815554560\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=818413568\n",
      "\t\tVirtual memory (bytes) snapshot=7960670208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio1-output...\n",
      "[\"Canada\"]\t1\n",
      "[\"Guam\"]\t3\n",
      "[\"Guinea\"]\t1\n",
      "[\"Portugal\"]\t1\n",
      "[\"Qatar\"]\t1\n",
      "[\"Somalia\"]\t1\n",
      "[\"South Africa\"]\t1\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\n",
      "[\"South Sudan\"]\t1\n",
      "[\"Spain\"]\t3\n",
      "[\"Turkey\"]\t1\n",
      "[\"United States\"]\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio1.root.20180116.193036.744895...\n",
      "Removing temp directory /tmp/mrjob-ejercicio1.root.20180116.193036.744895...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio1.py hdfs:///tmp/master-map-reduce-ejercicio1/* --output-dir hdfs:///tmp/carpeta/ejercicio1-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Canada\"]\t1\r\n",
      "[\"Guam\"]\t3\r\n",
      "[\"Guinea\"]\t1\r\n",
      "[\"Portugal\"]\t1\r\n",
      "[\"Qatar\"]\t1\r\n",
      "[\"Somalia\"]\t1\r\n",
      "[\"South Africa\"]\t1\r\n",
      "[\"South Georgia and the South Sandwich Islands\"]\t1\r\n",
      "[\"South Sudan\"]\t1\r\n",
      "[\"Spain\"]\t3\r\n",
      "[\"Turkey\"]\t1\r\n",
      "[\"United States\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio1-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio2.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield [country[1]], value[1]\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield None, [sum(values), key]\n",
    " \n",
    "  def reducer_third(self, _, values):\n",
    "    yield max(values)\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second),\n",
    "        MRStep(reducer=self.reducer_third)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 3...\n",
      "Creating temp directory /tmp/mrjob-ejercicio2.root.20180116.193149.935061\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/mrjob-ejercicio2.root.20180116.193149.935061/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio2.root.20180116.193149.935061...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio2.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal_ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal_ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 19:06 /tmp/master-map-reduce-ejercicio2/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 19:06 /tmp/master-map-reduce-ejercicio2/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/ejercicio2-output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /tmp/carpeta/ejercicio2-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio2.root.20180116.193241.900096\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180116.193241.900096/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob4591846438752512618.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516130695543_0003\n",
      "  Submitted application application_1516130695543_0003\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0003/\n",
      "  Running job: job_1516130695543_0003\n",
      "  Job job_1516130695543_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180116.193241.900096/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=503417\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18612224\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2678784\n",
      "\t\tTotal time spent by all map tasks (ms)=18176\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18176\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2616\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2616\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18176\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2616\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3620\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=937\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1495896064\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=1847590912\n",
      "\t\tVirtual memory (bytes) snapshot=10611372032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob4699910255818137611.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516130695543_0004\n",
      "  Submitted application application_1516130695543_0004\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0004/\n",
      "  Running job: job_1516130695543_0004\n",
      "  Job job_1516130695543_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0004 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180116.193241.900096/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367773\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7796736\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2719744\n",
      "\t\tTotal time spent by all map tasks (ms)=7614\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7614\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2656\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2656\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7614\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2656\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=206\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=809619456\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=815267840\n",
      "\t\tVirtual memory (bytes) snapshot=7962419200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob7924163414721403560.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516130695543_0005\n",
      "  Submitted application application_1516130695543_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0005/\n",
      "  Running job: job_1516130695543_0005\n",
      "  Job job_1516130695543_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0005 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio2-output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=367689\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=791\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9733120\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3535872\n",
      "\t\tTotal time spent by all map tasks (ms)=9505\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9505\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3453\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3453\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9505\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3453\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2040\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=237\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=805933056\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=822607872\n",
      "\t\tVirtual memory (bytes) snapshot=7962796032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio2-output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio2.root.20180116.193241.900096...\n",
      "Removing temp directory /tmp/mrjob-ejercicio2.root.20180116.193241.900096...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio2.py hdfs:///tmp/master-map-reduce-ejercicio2/* --output-dir hdfs:///tmp/carpeta/ejercicio2-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio2-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio3.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "    if len(splits) == 2: # datos de paises\n",
    "      symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "      country2digit = splits[1]\n",
    "      yield (country2digit, [symbol, splits[0]])\n",
    "    else: #  datos de personas\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      if (splits[1] == \"bueno\"):\n",
    "          value = 1\n",
    "          yield (country2digit, [symbol, value])\n",
    "\n",
    "  def reducer_first(self, key, values):\n",
    "    countries = [] # paises primero ya que llevan la clave 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield [country[1]], value[1]\n",
    "  \n",
    "  def reducer_second(self, key, values):\n",
    "    yield None, [sum(values), key]\n",
    " \n",
    "  def reducer_third(self, key, values):\n",
    "        valuesAgain = []\n",
    "        maxq = 1\n",
    "        for value in values:\n",
    "            if value[0] >= maxq:\n",
    "                maxq = value[0]\n",
    "            valuesAgain.append(value)\n",
    "        for value in valuesAgain:\n",
    "            if value[0] >= maxq:\n",
    "                yield maxq, value[1]\n",
    "        \n",
    "  def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper,\n",
    "        reducer=self.reducer_first),\n",
    "        MRStep(reducer=self.reducer_second),\n",
    "        MRStep(reducer=self.reducer_third)\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 3...\n",
      "Creating temp directory /tmp/mrjob-ejercicio3.root.20180116.193439.163480\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/mrjob-ejercicio3.root.20180116.193439.163480/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio3.root.20180116.193439.163480...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio3.py /media/notebooks/mrjob/mrjob/ejercicio1/countries.csv  /media/notebooks/mrjob/mrjob/ejercicio1/clients.csv > outputlocal_ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\r\n",
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! tail outputlocal_ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put /media/notebooks/mrjob/mrjob/ejercicio1/*.csv  /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1289 2018-01-15 20:18 /tmp/master-map-reduce-ejercicio3/clients.csv\r\n",
      "-rw-r--r--   3 root supergroup       4120 2018-01-15 20:18 /tmp/master-map-reduce-ejercicio3/countries.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls  /tmp/master-map-reduce-ejercicio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/ejercicio3-output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /tmp/carpeta/ejercicio3-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio3.root.20180116.193526.777673\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180116.193526.777673/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob6199562429364234069.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1516130695543_0006\n",
      "  Submitted application application_1516130695543_0006\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0006/\n",
      "  Running job: job_1516130695543_0006\n",
      "  Job job_1516130695543_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0006 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180116.193526.777673/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=257\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=503417\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7180\n",
      "\t\tHDFS: Number of bytes written=257\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25719808\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2721792\n",
      "\t\tTotal time spent by all map tasks (ms)=25117\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25117\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2658\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2658\n",
      "\t\tTotal vcore-seconds taken by all map tasks=25117\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2658\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3480\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1044\n",
      "\t\tInput split bytes=355\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1503514624\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=16\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=1836580864\n",
      "\t\tVirtual memory (bytes) snapshot=10610966528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob5562580084587714432.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516130695543_0007\n",
      "  Submitted application application_1516130695543_0007\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0007/\n",
      "  Running job: job_1516130695543_0007\n",
      "  Job job_1516130695543_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0007 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180116.193526.777673/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=386\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=311\n",
      "\t\tFILE: Number of bytes written=367773\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=716\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9527296\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2842624\n",
      "\t\tTotal time spent by all map tasks (ms)=9304\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9304\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2776\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2776\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9304\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2776\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=234\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=16\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=317\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=813744128\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=317\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=818413568\n",
      "\t\tVirtual memory (bytes) snapshot=7961088000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob1906530042329291121.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1516130695543_0008\n",
      "  Submitted application application_1516130695543_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1516130695543_0008/\n",
      "  Running job: job_1516130695543_0008\n",
      "  Job job_1516130695543_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1516130695543_0008 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/ejercicio3-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=367689\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=791\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8529920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2706432\n",
      "\t\tTotal time spent by all map tasks (ms)=8330\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8330\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2643\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2643\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8330\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2643\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2250\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=439\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1001918464\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=1269301248\n",
      "\t\tVirtual memory (bytes) snapshot=7961874432\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///tmp/carpeta/ejercicio3-output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\n",
      "3\t[\"Spain\"]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio3.root.20180116.193526.777673...\n",
      "Removing temp directory /tmp/mrjob-ejercicio3.root.20180116.193526.777673...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio3.py hdfs:///tmp/master-map-reduce-ejercicio3/* --output-dir hdfs:///tmp/carpeta/ejercicio3-output -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Guam\"]\r\n",
      "3\t[\"Spain\"]\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -tail /tmp/carpeta/ejercicio3-output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
